{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-uruguay",
   "metadata": {},
   "source": [
    "# RGI14 (South Asia, West)\n",
    "\n",
    "F. Maussion & S. Galos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from utils import mkdir, submission_summary, needs_size_filter, size_filter, plot_map, plot_date_hist\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-ensemble",
   "metadata": {},
   "source": [
    "### specify RGI-region and storage paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region of interest\n",
    "reg = 14\n",
    "\n",
    "# go down from rgi7_scripts/workflow\n",
    "data_dir = '../../rgi7_data/'\n",
    "\n",
    "# Level 2 GLIMS files\n",
    "l2_dir = os.path.join(data_dir, 'l2_sel_reg_tars')\n",
    "\n",
    "# Output directories\n",
    "output_dir = mkdir(os.path.join(data_dir, 'l3_rgi7a'))\n",
    "output_dir_tar = mkdir(os.path.join(data_dir, 'l3_rgi7a_tar'))\n",
    "\n",
    "# RGI v6 file for comparison later \n",
    "rgi6_reg_file = os.path.join(data_dir, 'l0_RGIv6', '14_rgi60_SouthAsiaWest.zip')\n",
    "\n",
    "# Region is based on GAMDAM, use for comparison\n",
    "support_dir = os.path.join(data_dir, 'l0_support_data')\n",
    "gamdam_dir = os.path.join(support_dir, 'gamdam')\n",
    "\n",
    "# Region file to select from gamdam\n",
    "reg_file = os.path.join(data_dir, 'l0_regions', '00_rgi70_regions', '00_rgi70_O1Regions.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-choir",
   "metadata": {},
   "source": [
    "### Load the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read L2 files\n",
    "shp = gpd.read_file('tar://' + l2_dir + f'/RGI{reg:02d}.tar.gz/RGI{reg:02d}/RGI{reg:02d}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581546a-c820-4da4-a359-f6846322cfe6",
   "metadata": {},
   "source": [
    "### List of submissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d52171-eca9-4f57-a474-9d61854a0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf, _ = submission_summary(shp)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ccf71-d217-41ac-a363-3d2f3d4dc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: write out selection in intermediate shape files for manual GIS review\n",
    "# tmp_output_dir = mkdir(os.path.join(data_dir, 'l0_tmp_data', f'rgi{reg:02d}_inventories'))\n",
    "# tmp_output_dir_tar = mkdir(os.path.join(data_dir, 'l0_tmp_data'))\n",
    "# for subid in shp.subm_id.unique():\n",
    "#     s_loc = shp.loc[shp.subm_id == subid]\n",
    "#     s_loc.to_file(tmp_output_dir + f'/subm_{int(subid):03d}.shp')\n",
    "# print('Taring...')\n",
    "# print(subprocess.run(['tar', '-zcvf', f'{tmp_output_dir_tar}/rgi{reg:02d}_inventories.tar.gz', '-C', \n",
    "#                       os.path.join(data_dir, 'l0_tmp_data'), f'rgi{reg:02d}_inventories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be019a10-8a4f-48df-bf54-b7ad509785df",
   "metadata": {},
   "source": [
    "## Outline selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e474474-bef9-4005-bbc0-7a7d994cd24e",
   "metadata": {},
   "source": [
    "### Apply selection criteria to create the RGI-07 data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab857d7-fc58-4222-8eb9-25b292a330cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to get the data relevant for RGI07 and select by attributes\n",
    "rgi7 = shp.loc[shp['subm_id'] == 698].copy()\n",
    "rgi7['is_rgi6'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27d906-ee1c-4e85-997d-7565052609f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size filter?\n",
    "needs_size_filter(rgi7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67e71b-9fbb-40e1-ae85-43722094297b",
   "metadata": {},
   "source": [
    "### Some sanity checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c572bb2-01c4-486c-bc2d-073aa770ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf, df_class = submission_summary(rgi7)\n",
    "df_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c441a-d802-458a-881d-dedfb8c52f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the orphaned rock outcrops\n",
    "orphan_f = os.path.join(data_dir, 'l1_orphan_interiors', f'RGI{reg:02d}', f'RGI{reg:02d}.shp')\n",
    "if os.path.exists(orphan_f):\n",
    "    orphan_f = gpd.read_file(orphan_f)\n",
    "    check = np.isin(rgi7.subm_id.unique(), orphan_f.subm_id.unique())\n",
    "    if np.any(check):\n",
    "        print(f'Orphan rock outcrops detected in subm_id {rgi7.subm_id.unique()[check]}')\n",
    "        orphan_f['area'] = orphan_f.to_crs({'proj':'cea'}).area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d6ca8-9a90-46ab-966f-57be699dfd1b",
   "metadata": {},
   "source": [
    "### Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2f3d0-2c55-4761-89ea-ef7e095be8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(rgi7, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9456c-2bf3-4f4d-81b5-01a4956675ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(rgi7, reg, is_rgi6=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745dc5a-cbaa-448e-9483-447fc41eb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_hist(rgi7, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d23ce-4e8e-4ae9-b291-ebf76a8ac1b2",
   "metadata": {},
   "source": [
    "### Text for github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca312ec9-3b47-49eb-b6ee-f20890e9e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgh = sdf.T\n",
    "fgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff4464-0d94-4c37-b22f-520269a89035",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fgh.to_markdown(headers=np.append(['subm_id'], fgh.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021503f-6c94-4e2b-992c-bdd1ab6f8bb9",
   "metadata": {},
   "source": [
    "## Write out and tar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464375e4-4049-44e9-a8ce-f5966169eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = mkdir(f'{output_dir}/RGI{reg:02d}/', reset=True)\n",
    "\n",
    "print('Writing...')\n",
    "rgi7.to_file(dd + f'RGI{reg:02d}.shp')\n",
    "\n",
    "print('Taring...')\n",
    "print(subprocess.run(['tar', '-zcvf', f'{output_dir_tar}/RGI{reg:02d}.tar.gz', '-C', output_dir, f'RGI{reg:02d}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326e16b-7622-4db4-aa77-244e77ec9aa3",
   "metadata": {},
   "source": [
    "## New RGI-file created - Check result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ac526-d76d-473b-ac91-ee66a22e4b8a",
   "metadata": {},
   "source": [
    "load reference data (here GAMDAM original) to enable comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae420d8-3885-4da2-a45d-28f1f93d5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to reference data set\n",
    "import glob\n",
    "from utils import open_zip_shapefile\n",
    "gam_files = glob.glob(gamdam_dir + '/*.zip')\n",
    "df_ref = []\n",
    "for gf in gam_files:\n",
    "    df_ref.append(open_zip_shapefile(gf))\n",
    "\n",
    "df_ref = pd.concat(df_ref).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7e565-f5c5-417d-9ef2-9bca890cb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate representative points for reference data\n",
    "ref_rp = df_ref.representative_point()\n",
    "\n",
    "# Make a dataframe out of it and add the original index to recover it later\n",
    "ref_rp = ref_rp.to_frame('geometry')\n",
    "ref_rp['orig_index'] = df_ref.index\n",
    "\n",
    "# Read region file\n",
    "reg_f = gpd.read_file(reg_file)\n",
    "\n",
    "# Make the overlay with the RGI region of interest -> get the rep. points which are located inside the region boundaries\n",
    "ref_intersect = gpd.overlay(ref_rp, reg_f.loc[reg_f.RGI_CODE == f'{reg:02d}'], how='intersection')\n",
    "\n",
    "# Now select the entries which intersect from the original shape file (-> extract the polygons) \n",
    "df_ref = df_ref.loc[ref_intersect['orig_index'].values].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3d1b2-51d7-4ee7-a90a-f8f476e666a5",
   "metadata": {},
   "source": [
    "### Number of elements (differences do not necessarily depict major problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f3efd-a941-43d1-9a83-50dec42a0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of glaciers in new RGI subset:', len(rgi7))\n",
    "print('Number of glaciers in reference data:', len(df_ref))\n",
    "print('Difference:', len(rgi7)-len(df_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53bab4-c24b-4f20-aca3-bbebfc00b10b",
   "metadata": {},
   "source": [
    "### Total area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11eac2-bd57-45d5-991c-ec5c39a4aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an area field to the selected GAMDAM table\n",
    "df_ref['area'] = df_ref.to_crs({'proj':'cea'}).area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fdd7a-476e-40ff-9d3f-b055f62844ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and compare area values\n",
    "Area_RGI = rgi7['area'].sum() * 1e-6\n",
    "print('Area RGI [km²]:', Area_RGI)\n",
    "Area_ref = df_ref['area'].sum() * 1e-6\n",
    "print('Area Ref [km²]:', Area_ref)\n",
    "d = (Area_RGI - Area_ref)\n",
    "print('Area difference [km²]:',d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ea0d4-65f8-413c-a341-a2413e6129db",
   "metadata": {},
   "source": [
    "**We believe that remaining errors are of the same type as Region 01: multipolygons that weren't properly ingested in GLIMS.** We will check this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73524601-c51b-4d45-b1ac-38dbee28605b",
   "metadata": {},
   "source": [
    "## Find missing glaciers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa659a-f0b0-42fa-a2a0-bd49eaf7e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import haversine\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778ee1a-672e-4f78-a820-9f352d4f25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_coord(geom):\n",
    "    \"\"\"To compute CenLon CenLat ourselves\"\"\"\n",
    "    x, y = geom.xy\n",
    "    return x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32041df6-7c39-4bc3-9adf-d7f8eaf04a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CenLon CenLat ourselves\n",
    "rp = df_ref.representative_point()\n",
    "\n",
    "coordinates = np.array(list(rp.apply(xy_coord)))\n",
    "df_ref['CenLon'] = coordinates[:, 0]\n",
    "df_ref['CenLat'] = coordinates[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03394f32-d520-4ed8-83e5-d4107d6dbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_orig = df_ref.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658b330-4bd1-4b11-ae16-5326794a57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all RGI7 glaciers and find their equivalent in ref\n",
    "df_ref = df_ref_orig.copy()\n",
    "not_found = {}\n",
    "to_drop = []\n",
    "for i, (ref_area, lon, lat) in progressbar.progressbar(enumerate(zip(rgi7['area'].values, rgi7.CenLon.values, rgi7.CenLat.values)), max_value=len(rgi7)):\n",
    "#     dist = haversine(lon, lat, df_ref.CenLon.values, df_ref.CenLat.values)\n",
    "    dist = (lon - df_ref.CenLon.values)**2 + (lat - df_ref.CenLat.values)**2 \n",
    "    found = False\n",
    "    for j in np.argsort(dist)[:10]:\n",
    "        s6 = df_ref.iloc[j]\n",
    "        if np.allclose(s6['area'], ref_area, rtol=0.001):\n",
    "            found = True\n",
    "            to_drop.append(s6.name)\n",
    "            break\n",
    "    if not found:\n",
    "        not_found[i] = df_ref.iloc[np.argsort(dist)[:10]]\n",
    "    if len(to_drop) > 1000:\n",
    "        df_ref.drop(labels=to_drop, inplace=True)\n",
    "        to_drop = []\n",
    "df_ref.drop(labels=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50830b84-e17c-4552-81bc-2857096624dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(not_found), len(df_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18feb5ad-065a-4f85-b147-ae07d657f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_rgi7 = rgi7.iloc[list(not_found.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd8a6f-6daa-41bd-acbc-60a1775f352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(not_found.keys()):\n",
    "    ax = rgi7.iloc[[k]].plot(edgecolor='k');\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c667fc0-d2ad-47bb-86a6-4ebbbfcb84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories\n",
    "output_dir = mkdir(os.path.join(data_dir, 'l3_problem_glaciers'))\n",
    "output_dir_tar = mkdir(os.path.join(data_dir, 'l3_problem_glaciers_tar'))\n",
    "\n",
    "dd = mkdir(f'{output_dir}/RGI{reg:02d}/', reset=True)\n",
    "\n",
    "print('Writing...')\n",
    "pb_rgi7.to_file(dd + f'RGI{reg:02d}_glims.shp')\n",
    "df_ref.to_file(dd + f'RGI{reg:02d}_ref.shp')\n",
    "\n",
    "print('Taring...')\n",
    "print(subprocess.run(['tar', '-zcvf', f'{output_dir_tar}/RGI{reg:02d}.tar.gz', '-C', output_dir, f'RGI{reg:02d}']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
