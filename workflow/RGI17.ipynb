{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocal-wrist",
   "metadata": {},
   "source": [
    "# RGI-07: Region 17 (Southern Andes)\n",
    "##### F. Roura November 2021\n",
    "\n",
    "Goal: compare L2 GLIMS files to original inventory to check possible errors in GLIMS ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-antique",
   "metadata": {},
   "source": [
    "## Files and storage paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region of interest\n",
    "reg = 17\n",
    "\n",
    "# go down from rgi7_scripts/workflow\n",
    "data_dir = '../../rgi7_data/'\n",
    "\n",
    "# Level 2 GLIMS files\n",
    "l2_dir = os.path.join(data_dir, 'l2_sel_reg_tars')\n",
    "\n",
    "# Output directories\n",
    "output_dir = mkdir(os.path.join(data_dir, 'l3_rgi7a'))\n",
    "output_dir_tar = mkdir(os.path.join(data_dir, 'l3_rgi7a_tar'))\n",
    "\n",
    "# Original inventory for GLIMS check \n",
    "ref_reg_file = os.path.join(data_dir, 'l0_support_data', 'Shape_Inventario_de_Glaciares.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-gathering",
   "metadata": {},
   "source": [
    "### Load the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read L2 files from GLIMS\n",
    "shp = gpd.read_file('tar://' + l2_dir + f'/RGI{reg:02d}.tar.gz/RGI{reg:02d}/RGI{reg:02d}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd82380-d34d-495e-96a7-df87bdfedf83",
   "metadata": {},
   "source": [
    "## List of submissions in GLIMS L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e086acc-4f4e-4e15-ba52-77f381850d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "toprint = []\n",
    "for subid in shp.subm_id.unique():\n",
    "    s_loc = shp.loc[shp.subm_id == subid]\n",
    "    s = ''\n",
    "    for c in ['subm_id', 'analysts', 'src_date']:\n",
    "        toprint = s_loc[c].unique()\n",
    "        if c != 'src_date':\n",
    "            s += ' ' + (str(toprint[0]))\n",
    "        else:\n",
    "            for d in toprint:\n",
    "                s += ' ' + d[:4]\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-industry",
   "metadata": {},
   "source": [
    "## Apply selection criteria to compare Glims data set to the original one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-cabin",
   "metadata": {},
   "source": [
    "### Step 1: extract ingested inventory from GLIMS data and do a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...extract l2 from GLIMS based on 'sumb_id'\n",
    "RGI_ss = shp.loc[shp['subm_id'] == 730].copy() #barcaza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e61de6-1113-470e-a00c-c27163c15cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out selection in intermediate shape files for manual GIS review\n",
    "#tmp_output_dir = mkdir(os.path.join(data_dir, 'l0_tmp_data', 'rgi17_inventories'))\n",
    "#tmp_output_dir_tar = mkdir(os.path.join(data_dir, 'l0_tmp_data'))\n",
    "#RGI_ss.to_file(tmp_output_dir + f'/subm_{int(730):03d}.shp')\n",
    "#print('Taring...')\n",
    "#print(subprocess.run(['tar', '-zcvf', f'{tmp_output_dir_tar}/rgi17_inventories.tar.gz', '-C', \n",
    "#                      os.path.join(data_dir, 'l0_tmp_data'), 'rgi17_inventories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-wallace",
   "metadata": {},
   "source": [
    "#### load reference data (here original inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to know the name of the file to open from zip\n",
    "import zipfile\n",
    "with zipfile.ZipFile(ref_reg_file, \"r\") as z:\n",
    "    for f in z.filelist:\n",
    "        if 'Inventario_de_Glaciares.shp' in f.filename:\n",
    "            if '.shp.xml' in f.filename:\n",
    "                break\n",
    "            else:\n",
    "                fname = f.filename\n",
    "\n",
    "# load reference data\n",
    "ref_odf = gpd.read_file('zip://' + ref_reg_file + '/' + fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-college",
   "metadata": {},
   "source": [
    "#### Number of elements (differences do not necessarily depict major problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of glaciers in new RGI subset:', len(RGI_ss))\n",
    "print('Number of glaciers in reference data:', len(ref_odf))\n",
    "print('Difference:', len(RGI_ss)-len(ref_odf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844679e3-08ee-4b48-aa8d-8b0e2b9b55b9",
   "metadata": {},
   "source": [
    "### Total area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3aa7d2-6c2d-42f1-9b08-7f47b2b813eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an area field to RGI_ss and reference data\n",
    "RGI_ss['area'] = RGI_ss.to_crs({'proj':'cea'}).area\n",
    "ref_odf['area'] = ref_odf.to_crs({'proj':'cea'}).area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4d91d-8f14-453c-91a9-81784f3e235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and compare area values\n",
    "Area_Rep = RGI_ss['area'].sum()/1000000\n",
    "print('Area Rep [km²]:', Area_Rep)\n",
    "Area_RGI6 = ref_odf['area'].sum()/1000000\n",
    "print('Area RGI6 [km²]:', Area_RGI6)\n",
    "d = (Area_Rep - Area_RGI6)\n",
    "d_perc = (d/Area_Rep*100)\n",
    "print('Area difference [km²]:',d,'/','percentage:', d_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283dc815-057f-415e-a635-5950021616fe",
   "metadata": {},
   "source": [
    "### result of check (RGI from GLIMS L2 and original inventory):\n",
    "#### difference in number of glaciers: 516\n",
    "#### duplicate IDs: 0 in GLIMS, 438 in original inventory\n",
    "#### area difference: 41 km² / 0.17 %\n",
    "#### general comment: in general GLIMS ingestion works but differences need more detailed check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2468f9-a71e-4d74-b259-bcdadd8d88c0",
   "metadata": {},
   "source": [
    "## ............| Extra analysis, find repeated IDs, find possible missing glaciers,... starts here |..................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-configuration",
   "metadata": {},
   "source": [
    "#### check for duplicate glacier IDs: many glaciers have shared id in reference data set. Let's see how many of them are actually different glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8a58b-bfce-495a-9eea-692d72281d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Duplicate IDs in original:', len(ref_odf)-len(ref_odf['OBJECTID'].unique()))\n",
    "print('Duplicate IDs in GLIMS:', len(RGI_ss)-len(RGI_ss['glac_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015f847-1c73-4cb7-afdf-19932110c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(len(ref_odf['OBJECTID'].unique())-len(ref_odf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74768d-d188-4994-8d9d-88763e56e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## subset ref_odf to work with the subset that has not unique IDs\n",
    "rep_id = gpd.geodataframe.GeoDataFrame()\n",
    "for idi in ref_odf['OBJECTID'].unique():\n",
    "    if len(ref_odf.loc[ref_odf['OBJECTID']==idi]) > 1:\n",
    "        rep_id = pd.concat([rep_id, ref_odf.loc[ref_odf['OBJECTID']==idi]])\n",
    "        print(len(ref_odf.loc[ref_odf['OBJECTID']==idi]), idi)\n",
    "rep_id['OBJECTID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40f962-67c3-4f95-96d9-503b5c49332e",
   "metadata": {},
   "source": [
    "### **?** I don't know why do we have length 445 but 438 repeated IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94ef74-14f6-4584-a7bc-8b99aa6d3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeated Ids:\n",
    "tmp_ids = rep_id['OBJECTID'].unique()\n",
    "\n",
    "## find if the glaciers with same id are actually the same glacier or not:\n",
    "for tmp in tmp_ids:\n",
    "    tmp_set = rep_id.loc[rep_id['OBJECTID'] == tmp]\n",
    "    \n",
    "## number of total glaciers with reperated IDs but different other attributes:\n",
    "    print( 'Repeated Id: ', tmp, '; glaciers with this id based on \"geometry\": ', len(tmp_set['geometry'].unique()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172343b-7c72-4344-846a-c0c1dc0f380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_id.loc[rep_id[\"OBJECTID\"]==4201].plot() # --> repeated ID not repeated geometry. (different geometries with same ID)\n",
    "rep_id.loc[rep_id[\"OBJECTID\"]==4721].plot() # --> repeated ID not repeated geometry. (different geometries with same ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e1e9b-647f-4b4e-8f4b-f68041c165ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_id.loc[rep_id[\"OBJECTID\"]==330].plot() # --> repeated ID and geometry. (glacier with ID=330 is twice in the dataset)\n",
    "rep_id.loc[rep_id[\"OBJECTID\"]==1364].plot() #--> repeated ID and geometry. (glacier with ID=1364 is twice in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bddeb0-6b9c-4234-b4d5-2aac766780cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_id.loc[rep_id[\"OBJECTID\"]==1996].plot() # --> repeated ID not repeated geometry. (different geometries with same ID)\n",
    "rep_id.loc[rep_id[\"OBJECTID\"]==6559].plot() # --> repeated ID not repeated geometry. (different geometries with same ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06bd6a-5f93-4b86-915b-e15da27d2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare all the elements inside the repeated id == 0\n",
    "## many loops and not the \"pythonic way\", but it's how i managed to do it...\n",
    "rep_id0 = rep_id.loc[rep_id[\"OBJECTID\"] == 0]\n",
    "for id0 in range(0,len(rep_id0)): ## loop all entries\n",
    "    tmp = list(range(0,id0)) + list(range(id0+1,len(rep_id0))) ## all entries minus the current one\n",
    "    for item in tmp: ## loop all entries except the current one\n",
    "        alltrue = sum(np.array(rep_id.iloc[item]) == np.array(rep_id.iloc[id0]))\n",
    "        if alltrue == 67:\n",
    "            print('current equal glaciers in ids ', id0, ' and ', item)\n",
    "\n",
    "## we don't get any message, so there are not 2 identical entries in the id=0 subset\n",
    "## --> number of glaciers corresponding to id=0 is the length of the subset:\n",
    "len(rep_id0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22369c-47da-42cf-a4ec-a5e467f8f967",
   "metadata": {},
   "source": [
    "### Check nulls in ref_odf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587be745-26a3-4713-93c4-5d62c6f8ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_id.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca1722-f0e7-4408-9ce2-6c3ba0a6523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out selection in intermediate shape files for manual GIS review\n",
    "tmp_output_dir = mkdir(os.path.join(data_dir, 'l0_tmp_data', 'rgi17_inventories'))\n",
    "tmp_output_dir_tar = mkdir(os.path.join(data_dir, 'l0_tmp_data'))\n",
    "rep_id0.to_file(tmp_output_dir + f'/subm_{int(730):03d}_0.shp')\n",
    "print('Taring...')\n",
    "print(subprocess.run(['tar', '-zcvf', f'{tmp_output_dir_tar}/rgi17_inventories.tar.gz', '-C', \n",
    "                      os.path.join(data_dir, 'l0_tmp_data'), 'rgi17_inventories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f9fb9-5fa7-4ae7-a99c-6757bdd1a1ce",
   "metadata": {},
   "source": [
    "## we have 432 glaciers with repeated id=0 (+432), 2 glaciers repeated (identical entries) (-2), 4 repeated ids that represent different polygons (2+1+1+1 = +5) --> total glaciers are \"non repeated\" + 432+2+5=439\n",
    "\n",
    "### Problem: 439 vs 438 vs 432 --> look at it in more detail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
