{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adc67a-c914-4b66-a3de-aed2506fb298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import shapely.geometry as shpg\n",
    "from shapely.ops import linemerge\n",
    "from shapely import set_precision\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5729c-29eb-4fb6-80cf-b7d279f0343a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62aa692-c347-4e74-bc0f-c0cdc9da4669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd() + '/../..')\n",
    "from utils import mkdir, open_zip_shapefile, open_tar_shapefile, haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c601bdf-1db0-429e-bb22-479f14ffe496",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger('papermill')\n",
    "logging.basicConfig(level='INFO', format=\"%(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487f7be-608e-44f1-88ec-671465325a11",
   "metadata": {},
   "source": [
    "## Files and storage paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c753f79-c5b6-49f0-b6ba-bfe40fd7eac3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Region of interest\n",
    "reg = 1\n",
    "\n",
    "# go down from rgi7_scripts/workflow\n",
    "data_dir = '../../../../rgi7_data/'\n",
    "\n",
    "# Input dirctory\n",
    "input_dir = os.path.join(data_dir, 'l3_rgi7a_tar')\n",
    "\n",
    "# Output directories\n",
    "output_dir = mkdir(os.path.join(data_dir, 'l4_rgi7b0'))\n",
    "output_dir_tar = mkdir(os.path.join(data_dir, 'l4_rgi7b0_tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d31af4-b2b2-4dc9-b286-be594f2feb7e",
   "metadata": {},
   "source": [
    "### Load the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab54b7-92c0-45fa-a943-25225b119001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "shp = open_tar_shapefile(input_dir + f'/RGI{reg:02d}.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf5415-997a-4aab-be08-726242a61228",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_attrs = pd.DataFrame(shp.drop('geometry', axis=1))\n",
    "orig_attrs.T;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b92b0a-3767-455a-8884-4666b0cf504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'conn_lvl' not in shp:\n",
    "    print('Add conn_lvl')\n",
    "    shp['conn_lvl'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e91a0-6561-4cd7-907a-60aeb23e545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = shp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17837ca6-2d55-4ae7-af15-4b572f2eefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {'area':'area_km2', 'CenLon': 'cenlon', 'CenLat': 'cenlat', 'glac_id':'glims_id'}\n",
    "odf = odf.rename(rename, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2895b31-0bb5-4192-a158-c6ff07abd1ba",
   "metadata": {},
   "source": [
    "Recompute area and center point to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54183cb-e0e5-4e82-942d-dbde26f1e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_coord(geom):\n",
    "    x, y = geom.xy\n",
    "    return x[0], y[0]\n",
    "\n",
    "rp = odf.representative_point()\n",
    "\n",
    "coordinates = np.array(list(rp.apply(xy_coord)))\n",
    "odf['cenlon'] = coordinates[:, 0]\n",
    "odf['cenlat'] = coordinates[:, 1]\n",
    "\n",
    "odf['area_km2'] = odf.to_crs({'proj':'cea'}).area * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dff652-d75c-4702-90d0-8e2462577487",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['glac_name'] = odf['glac_name'].where(odf['glac_name'] != 'None', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88df3a-a0da-41aa-80a2-6ee799789b25",
   "metadata": {},
   "source": [
    "## Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9f86f-42fa-4ae4-b5f5-bbc2708d7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../rgi7_attributes_metadata.json', 'r') as infile:\n",
    "    meta = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe0d1f-3125-4062-b9c4-2d9717db2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_new = odf[[]].copy().reset_index(drop=True)\n",
    "\n",
    "for col, attrs in meta.items():\n",
    "    if col not in odf:\n",
    "        if attrs['datatype'] == 'str':\n",
    "            odf_new[col] = ''\n",
    "        elif attrs['datatype'] == 'float':\n",
    "            odf_new[col] = np.NaN\n",
    "        elif attrs['datatype'] == 'int':\n",
    "            odf_new[col] = -999 \n",
    "    else: \n",
    "        if attrs['datatype']:\n",
    "            odf_new[col] = odf[col].astype(attrs['datatype'])\n",
    "        else: \n",
    "            odf_new[col] = odf[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7932c-0795-487e-89d3-f72c20eeb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_new = gpd.GeoDataFrame(odf_new)\n",
    "odf_new.crs = odf.crs\n",
    "odf_new = odf_new.reset_index(drop=True)\n",
    "odf_new.iloc[:1].T;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e8521-6572-474c-afe5-0989eb843536",
   "metadata": {},
   "source": [
    "## Regions, subregions and RGI IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c32b7-a630-4f2b-a82d-5a2372ad527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_new['o1region'] = f'{reg:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc6a76-449e-41be-9f83-76f94e0aaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sreg_file = os.path.join(data_dir, '00_rgi70_regions', '00_rgi70_O2Regions')\n",
    "sreg = gpd.read_file(sreg_file)\n",
    "sreg = sreg.loc[sreg.o1region == f'{reg:02d}']\n",
    "sreg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29782e31-7feb-46f1-8398-16296d7dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseid = f'RGI2000-v7.0-G-{reg:02d}-'\n",
    "count = 0\n",
    "\n",
    "rp = odf_new.representative_point()\n",
    "rp = rp.to_frame('geometry')\n",
    "rp['orig_index'] = odf_new.index\n",
    "\n",
    "total = 0\n",
    "\n",
    "for i, s in sreg.iterrows():\n",
    "    \n",
    "    intersect = gpd.overlay(rp, sreg.loc[[i]], how='intersection')\n",
    "    odf_sreg = odf_new.loc[intersect['orig_index']].copy()\n",
    "    odf_sreg['o2region'] = s['o2region']\n",
    "    \n",
    "    total += len(odf_sreg)\n",
    "    \n",
    "    if len(odf_sreg) == 0:\n",
    "        # 19-05 Ross Ice Shelf one of them\n",
    "        continue\n",
    "    \n",
    "    # Ids generation\n",
    "    # Left most point and we start from there\n",
    "    slon, slat = odf_sreg.loc[odf_sreg.cenlon == odf_sreg.cenlon.min()][['cenlon', 'cenlat']].iloc[0].values\n",
    "    \n",
    "    todo = odf_sreg.index.values\n",
    "    todo_lon = odf_sreg['cenlon'].values\n",
    "    todo_lat = odf_sreg['cenlat'].values\n",
    "    ids = []\n",
    "    while len(todo) > 0:\n",
    "        dis = haversine(slon, slat, todo_lon, todo_lat)\n",
    "        idm = np.argmin(dis)\n",
    "        ids.append(todo[idm])\n",
    "        slon, slat = todo_lon[idm], todo_lat[idm]\n",
    "        todo = np.delete(todo, idm)\n",
    "        todo_lon = np.delete(todo_lon, idm)\n",
    "        todo_lat = np.delete(todo_lat, idm)\n",
    "    \n",
    "    assert len(ids) == len(odf_sreg)\n",
    "    odf_sreg = odf_sreg.loc[ids].copy()\n",
    "    \n",
    "    odf_sreg['rgi_id'] = [baseid + f'{l+count:05d}' for l in range(len(odf_sreg))]\n",
    "    count += len(odf_sreg)\n",
    "    odf_new.loc[odf_sreg.index, 'rgi_id'] = odf_sreg['rgi_id']\n",
    "    odf_new.loc[odf_sreg.index, 'o2region'] = odf_sreg['o2region']\n",
    "    \n",
    "odf_new = odf_new.sort_values(by='rgi_id').reset_index(drop=True)\n",
    "\n",
    "assert odf_new['o2region'].isnull().sum() == 0\n",
    "assert int(odf_new.iloc[-1]['rgi_id'].split('-')[-1]) == odf_new.iloc[-1].name\n",
    "if len(odf_new['o2region'].unique()) != len(sreg):\n",
    "    log.warning(f'RGI{reg:02d}: some subregions have NO glaciers in them')\n",
    "else:\n",
    "    log.info(f'RGI{reg:02d}: all subregions have glaciers in them')\n",
    "assert len(odf_new) == len(odf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e0186-4fa8-448b-83e3-f900ea245a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_new.iloc[:1].T;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c990a4-231d-4107-890d-a1bf505260e1",
   "metadata": {},
   "source": [
    "## Links to RGI6 (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8dc7d1-8bde-4a20-b62e-c424bb67436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import overlaps_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6976c-da1c-4542-853b-780e389d39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: script which uses overlaps_helpers (Author: Ethan Welty) to create a csv file containing the links to RGI6\n",
    "# It should then be written to disk as:\n",
    "# odf_links.to_csv(dd + f'RGI2000-v7.0-G-{reg_file.long_code}_links.csv', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad528b70-acfd-47db-99f3-3e123373516a",
   "metadata": {},
   "source": [
    "## Submission metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e8c16-42eb-443f-a176-c767ee7c3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../rgi7_submission_info_metadata.json', 'r') as infile:\n",
    "    meta_sub = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9d8ae-9743-4540-a5f4-50e44c0da289",
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_id = orig_attrs['subm_id'].unique()\n",
    "odf_subm = pd.DataFrame()\n",
    "for sid in subm_id:\n",
    "    sel = orig_attrs.loc[orig_attrs['subm_id'] == sid]\n",
    "    for k in meta_sub.keys():\n",
    "        if k == 'subm_id':\n",
    "            continue\n",
    "        attrs = meta_sub[k]\n",
    "        if k not in sel:\n",
    "            if attrs['datatype'] == 'str':\n",
    "                odf_subm.loc[int(sid), k] = ''\n",
    "            elif attrs['datatype'] == 'float':\n",
    "                odf_subm.loc[int(sid), k] = np.NaN\n",
    "            elif attrs['datatype'] == 'int':\n",
    "                odf_subm.loc[int(sid), k] = -999\n",
    "        else: \n",
    "            assert len(sel[k].unique()==1), f'{k} has non unique values'\n",
    "            if attrs['datatype']:\n",
    "                data = sel[k].astype(attrs['datatype']).iloc[0]\n",
    "            else: \n",
    "                data = sel[k].iloc[0]\n",
    "                \n",
    "            if attrs['datatype'] == 'str':\n",
    "                # Clean\n",
    "                data = data.strip().lstrip(';').strip()\n",
    "                \n",
    "            odf_subm.loc[int(sid), k] = data\n",
    "            \n",
    "    odf_subm.loc[int(sid), 'n_outlines'] = len(sel)\n",
    "    odf_subm.loc[int(sid), 'area_km2'] = sel['area'].sum() * 1e-6\n",
    "        \n",
    "odf_subm.index.name = 'subm_id' \n",
    "odf_subm['n_outlines'] = odf_subm['n_outlines'].astype(int)\n",
    "odf_subm['rc_id'] = odf_subm['rc_id'].astype(int)\n",
    "odf_subm = odf_subm.sort_index()\n",
    "odf_subm;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9da820-bb63-44ba-9c9e-4b9bb43dc83d",
   "metadata": {},
   "source": [
    "## Write out and tar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2ad69-a5e6-49ad-b2c3-7e928bbd2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_file = os.path.join(data_dir, '00_rgi70_regions', '00_rgi70_O1Regions')\n",
    "reg_file = gpd.read_file(reg_file)\n",
    "reg_file = reg_file.loc[reg_file.o1region == f'{reg:02d}'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280b37d-6683-4670-b499-181fa14b57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = mkdir(f'{output_dir}/RGI2000-v7.0-G-{reg_file.long_code}/', reset=True)\n",
    "\n",
    "print('Writing...')\n",
    "odf_new.to_file(dd + f'RGI2000-v7.0-G-{reg_file.long_code}.shp')\n",
    "odf_subm.to_csv(dd + f'RGI2000-v7.0-G-{reg_file.long_code}_subm_info.csv', quoting=csv.QUOTE_NONNUMERIC)\n",
    "odf_new.drop('geometry', axis=1).set_index('rgi_id').to_csv(dd + f'RGI2000-v7.0-G-{reg_file.long_code}_attributes.csv', quoting=csv.QUOTE_NONNUMERIC)\n",
    "shutil.copyfile('../README_tpl.md', dd + f'README.md')\n",
    "\n",
    "print('Taring...')\n",
    "print(subprocess.run(['tar', '-zcvf', f'{output_dir_tar}/RGI2000-v7.0-G-{reg_file.long_code}.tar.gz', '-C', output_dir, f'RGI2000-v7.0-G-{reg_file.long_code}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667dbd4-fc48-445c-bee8-6156c82bfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf_new;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64b5cc-046a-4d96-a3ce-d047d9e20a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "for f in glob.glob( f\"{output_dir_tar}/*.properties\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3a216-a8bf-4eb7-a485-7c38fc18066c",
   "metadata": {},
   "source": [
    "## Merged product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b1f55-7f57-4977-a70c-3e329f23d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GeoDataFrame from wich the merged product is created\n",
    "gdf = odf_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15462e-5669-4050-8a7a-f887b6ed58b0",
   "metadata": {},
   "source": [
    "### Create interesects shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9ff91-1767-40a6-8db3-b1522914aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how the output should look like\n",
    "odf_intersects_cols = ['rgi_id_1', 'rgi_id_2', 'geometry']\n",
    "odf_intersects = gpd.GeoDataFrame(columns=odf_intersects_cols)\n",
    "odf_intersects.crs = gdf.crs\n",
    "\n",
    "# here we can define a minimum length for a intersection line,\n",
    "# if None all intersects are included\n",
    "min_length = None\n",
    "\n",
    "# this precision is needed to avoid unwanted side\n",
    "# effects due to floating point representation of\n",
    "# polygon coordinates\n",
    "precision = 1e-10\n",
    "\n",
    "# this creates r-tree spatial indices for a fast search for potential intersects\n",
    "# e.g. see https://geoffboeing.com/2016/10/r-tree-spatial-index-python/\n",
    "spatial_index = gdf.sindex\n",
    "\n",
    "for _, major in gdf.iterrows():\n",
    "\n",
    "    # find possible intersects using spatial indexing\n",
    "    possible_intersects_index = list(spatial_index.query(major.geometry))\n",
    "    possible_intersects = gdf.iloc[possible_intersects_index]\n",
    "\n",
    "    # exclude the major geometry itself\n",
    "    possible_intersects = possible_intersects.loc[possible_intersects.rgi_id != major.rgi_id]\n",
    "\n",
    "    # run true intersection query only on possible intersects\n",
    "    actual_intersects = possible_intersects[possible_intersects.intersects(major.geometry)]\n",
    "\n",
    "    for _, neighbor in actual_intersects.iterrows():\n",
    "        # Already computed?\n",
    "        if neighbor.rgi_id in odf_intersects.rgi_id_1.values:\n",
    "            continue\n",
    "\n",
    "        # Here set new precision of geometries before intersecting,\n",
    "        # this avoids side effects due to floating point\n",
    "        # representation of coordinates (e.g. result is a polygon\n",
    "        # instead of a line)\n",
    "        mult_intersect = set_precision(major.geometry, precision).intersection(\n",
    "            set_precision(neighbor.geometry, precision))\n",
    "\n",
    "        # checks that floating point representation is ok\n",
    "        assert not isinstance(mult_intersect, shpg.Polygon)\n",
    "\n",
    "        if isinstance(mult_intersect, shpg.Point):\n",
    "            continue\n",
    "        if isinstance(mult_intersect, shpg.linestring.LineString):\n",
    "            mult_intersect = shpg.MultiLineString([mult_intersect])\n",
    "        if len(mult_intersect.geoms) == 0:\n",
    "            continue\n",
    "        mult_intersect = [m for m in mult_intersect.geoms if\n",
    "                          not isinstance(m, shpg.Point)]\n",
    "\n",
    "        # checks that floating point representation is ok\n",
    "        for m in mult_intersect:\n",
    "            assert not isinstance(m, shpg.Polygon)\n",
    "\n",
    "        if len(mult_intersect) == 0:\n",
    "            continue\n",
    "\n",
    "        # Simplify the geometries if possible\n",
    "        try:\n",
    "            mult_intersect = linemerge(mult_intersect)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        # Add each line to the output file\n",
    "        if isinstance(mult_intersect, shpg.linestring.LineString):\n",
    "            mult_intersect = shpg.MultiLineString([mult_intersect])\n",
    "        for line in mult_intersect.geoms:\n",
    "            assert isinstance(line, shpg.linestring.LineString)\n",
    "            # Filter the very small ones\n",
    "            if min_length is not None:\n",
    "                if lin.length < min_length:\n",
    "                    continue\n",
    "            line = gpd.GeoDataFrame([[major.rgi_id, neighbor.rgi_id, line]],\n",
    "                                    columns=odf_intersects_cols)\n",
    "            odf_intersects = pd.concat([odf_intersects, line])\n",
    "\n",
    "odf_intersects = odf_intersects.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a64c4-6428-4ab5-b8a8-b1004bac1607",
   "metadata": {},
   "source": [
    "### Create merged shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f2ff9-8f8e-4822-a705-91fecbf76023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge outlines\n",
    "odf_merged = gdf.dissolve().explode().reset_index()\n",
    "\n",
    "# drop attributes and set others to nan (just to be sure all attributes are recomputed)\n",
    "attributes_to_keep = ['rgi_id', 'o1region', 'area_km2', 'geometry']\n",
    "attributes_to_drop = [i for i in odf_merged.columns if i not in attributes_to_keep]\n",
    "odf_merged.drop(columns=attributes_to_drop, inplace=True)\n",
    "for attr in attributes_to_keep:\n",
    "    if attr not in ['o1region', 'geometry']:\n",
    "        odf_merged[attr] = np.nan\n",
    "\n",
    "# assign new ids for merged glacier complexes\n",
    "baseid = f'RGI2000-v7.0-C-{reg:02d}-'\n",
    "odf_merged['rgi_id'] = [baseid + f'{i:05d}' for i in odf_merged.index]\n",
    "\n",
    "# define new cenlon and cenlat\n",
    "def xy_coord(geom):\n",
    "    x, y = geom.xy\n",
    "    return x[0], y[0]\n",
    "\n",
    "rp = odf_merged.representative_point()\n",
    "coordinates = np.array(list(rp.apply(xy_coord)))\n",
    "odf_merged['cenlon'] = coordinates[:, 0]\n",
    "odf_merged['cenlat'] = coordinates[:, 1]\n",
    "\n",
    "# calculate new area\n",
    "odf_merged['area_km2'] = odf_merged.to_crs({'proj':'cea'}).area * 1e-6\n",
    "\n",
    "# check that total area is unchanged\n",
    "assert np.allclose(gdf.area_km2.sum(),\n",
    "                   odf_merged.area_km2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958e6ee-5dc2-4d00-a46e-b7129aeb3f67",
   "metadata": {},
   "source": [
    "### Create conversion list between individual glacier ids and glacier complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0328-6054-49e0-b8b9-fffefe529e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_ids_per_complex_dict = {}\n",
    "\n",
    "# this creates r-tree spatial indices for a fast search for potential overlaps\n",
    "# e.g. see https://geoffboeing.com/2016/10/r-tree-spatial-index-python/\n",
    "spatial_index = gdf.sindex\n",
    "\n",
    "for _, row in odf_merged.iterrows():\n",
    "    possible_overlap_index = list(spatial_index.query(row.geometry))\n",
    "    possible_overlaps = gdf.iloc[possible_overlap_index]\n",
    "    in_merged = [row.geometry.contains(shpg.Point(x, y)) for\n",
    "                 (x, y) in zip(possible_overlaps.cenlon, possible_overlaps.cenlat)]\n",
    "\n",
    "    individual_ids_per_complex_dict[row.rgi_id] = list(possible_overlaps.loc[in_merged].rgi_id.values)\n",
    "\n",
    "# check that every individual glacier was assigned to one and only one complex\n",
    "assigned_ids = [i for sublist in list(individual_ids_per_complex_dict.values())\n",
    "                for i in sublist]\n",
    "assert len(assigned_ids) == len(gdf.rgi_id)\n",
    "assert len(np.unique(assigned_ids)) == len(gdf.rgi_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e52ee-a2d5-4e38-886c-2b224dd70842",
   "metadata": {},
   "source": [
    "### Write out and tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287561f4-575e-4da7-8281-6b0e00370c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_file = os.path.join(data_dir, '00_rgi70_regions', '00_rgi70_O1Regions')\n",
    "reg_file = gpd.read_file(reg_file)\n",
    "reg_file = reg_file.loc[reg_file.o1region == f'{reg:02d}'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b7975a-d782-4279-99c1-882a9c8063a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = mkdir(f'{output_dir}/RGI2000-v7.0-C-{reg_file.long_code}/', reset=True)\n",
    "\n",
    "print('Writing...')\n",
    "\n",
    "# save intersects\n",
    "odf_intersects.to_file(dd + f'RGI2000-v7.0-I-{reg_file.long_code}.shp')\n",
    "\n",
    "# save merged product with attribute table\n",
    "odf_merged.to_file(dd + f'RGI2000-v7.0-C-{reg_file.long_code}.shp')\n",
    "odf_merged.drop('geometry', axis=1).set_index('rgi_id').to_csv(dd + f'RGI2000-v7.0-C-{reg_file.long_code}_attributes.csv', quoting=csv.QUOTE_NONNUMERIC)\n",
    "shutil.copyfile('../README_tpl.md', dd + f'README.md')\n",
    "\n",
    "# save conversion list between G and C\n",
    "fp = f'RGI2000-v7.0-C-to-G-{reg_file.long_code}.json'\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(individual_ids_per_complex_dict, f)\n",
    "\n",
    "print('Taring...')\n",
    "print(subprocess.run(['tar', '-zcvf', f'{output_dir_tar}/RGI2000-v7.0-C-{reg_file.long_code}.tar.gz', '-C', output_dir, f'RGI2000-v7.0-C-{reg_file.long_code}']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
